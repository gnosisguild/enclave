{"noir_version":"1.0.0-beta.16+2d46fca7203545cbbfb31a0d0328de6c10a8db95","hash":"11535691908104044916","abi":{"parameters":[{"name":"verification_key","type":{"kind":"array","length":115,"type":{"kind":"field"}},"visibility":"private"},{"name":"proofs","type":{"kind":"array","length":2,"type":{"kind":"array","length":457,"type":{"kind":"field"}}},"visibility":"private"},{"name":"commitments","type":{"kind":"array","length":2,"type":{"kind":"field"}},"visibility":"public"},{"name":"key_hash","type":{"kind":"field"},"visibility":"private"}],"return_type":{"abi_type":{"kind":"field"},"visibility":"public"},"error_types":{}},"bytecode":"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","debug_symbols":"tZTLboMwEEX/xWsWHj/G4/xKVUUkcSIkC5ADlaoo/15DMYSFEUrVlV/cMw9f/GAXd+pvx6q+Nnd2+HiwU6i8r25H35zLrmrquPt4Fiwtj11wLm6xl/Ooasvg6o4d6t77gn2Vvh8/urdlPY5dGeIpL5irL3GMwGvl3TB7Foua56Ua9STWRs1yDbv1xiS9hXf0Qkx65DKnl3k9aEETABDtQhArgsoTpCA5EaREnAlC7K0B9VwDvtMDRJX0ht7Rc5v0luf09Oce2v/sIYnkQyKRqwFgowhuZULEOS2NNGYNERudNDYxkFBnERuGNJzS72hiSguC1ogNRyLAbCjQy42S2p8FCEhZgNTZLHADgTLZyqCGbBYmj7CgEsKCVVnEhjcVQboRRXpxBtj9hZDEVAgZvcriM67KcxVWzzLj0XkFg8FIBRNDsILJ30GNoYdgoSpP3k3P+LWvzy+vevfdppP07rehObtLH9wQajyLwX8A","file_map":{"19":{"source":"// Exposed only for usage in `std::meta`\npub(crate) mod poseidon2;\n\nuse crate::default::Default;\nuse crate::embedded_curve_ops::{\n    EmbeddedCurvePoint, EmbeddedCurveScalar, multi_scalar_mul, multi_scalar_mul_array_return,\n};\nuse crate::meta::derive_via;\n\n#[foreign(sha256_compression)]\n// docs:start:sha256_compression\npub fn sha256_compression(input: [u32; 16], state: [u32; 8]) -> [u32; 8] {}\n// docs:end:sha256_compression\n\n#[foreign(keccakf1600)]\n// docs:start:keccakf1600\npub fn keccakf1600(input: [u64; 25]) -> [u64; 25] {}\n// docs:end:keccakf1600\n\npub mod keccak {\n    #[deprecated(\"This function has been moved to std::hash::keccakf1600\")]\n    pub fn keccakf1600(input: [u64; 25]) -> [u64; 25] {\n        super::keccakf1600(input)\n    }\n}\n\n#[foreign(blake2s)]\n// docs:start:blake2s\npub fn blake2s<let N: u32>(input: [u8; N]) -> [u8; 32]\n// docs:end:blake2s\n{}\n\n// docs:start:blake3\npub fn blake3<let N: u32>(input: [u8; N]) -> [u8; 32]\n// docs:end:blake3\n{\n    if crate::runtime::is_unconstrained() {\n        // Temporary measure while Barretenberg is main proving system.\n        // Please open an issue if you're working on another proving system and running into problems due to this.\n        crate::static_assert(\n            N <= 1024,\n            \"Barretenberg cannot prove blake3 hashes with inputs larger than 1024 bytes\",\n        );\n    }\n    __blake3(input)\n}\n\n#[foreign(blake3)]\nfn __blake3<let N: u32>(input: [u8; N]) -> [u8; 32] {}\n\n// docs:start:pedersen_commitment\npub fn pedersen_commitment<let N: u32>(input: [Field; N]) -> EmbeddedCurvePoint {\n    // docs:end:pedersen_commitment\n    pedersen_commitment_with_separator(input, 0)\n}\n\n#[inline_always]\npub fn pedersen_commitment_with_separator<let N: u32>(\n    input: [Field; N],\n    separator: u32,\n) -> EmbeddedCurvePoint {\n    let mut points = [EmbeddedCurveScalar { lo: 0, hi: 0 }; N];\n    for i in 0..N {\n        // we use the unsafe version because the multi_scalar_mul will constrain the scalars.\n        points[i] = from_field_unsafe(input[i]);\n    }\n    let generators = derive_generators(\"DEFAULT_DOMAIN_SEPARATOR\".as_bytes(), separator);\n    multi_scalar_mul(generators, points)\n}\n\n// docs:start:pedersen_hash\npub fn pedersen_hash<let N: u32>(input: [Field; N]) -> Field\n// docs:end:pedersen_hash\n{\n    pedersen_hash_with_separator(input, 0)\n}\n\n#[no_predicates]\npub fn pedersen_hash_with_separator<let N: u32>(input: [Field; N], separator: u32) -> Field {\n    let mut scalars: [EmbeddedCurveScalar; N + 1] = [EmbeddedCurveScalar { lo: 0, hi: 0 }; N + 1];\n    let mut generators: [EmbeddedCurvePoint; N + 1] =\n        [EmbeddedCurvePoint::point_at_infinity(); N + 1];\n    let domain_generators: [EmbeddedCurvePoint; N] =\n        derive_generators(\"DEFAULT_DOMAIN_SEPARATOR\".as_bytes(), separator);\n\n    for i in 0..N {\n        scalars[i] = from_field_unsafe(input[i]);\n        generators[i] = domain_generators[i];\n    }\n    scalars[N] = EmbeddedCurveScalar { lo: N as Field, hi: 0 as Field };\n\n    let length_generator: [EmbeddedCurvePoint; 1] =\n        derive_generators(\"pedersen_hash_length\".as_bytes(), 0);\n    generators[N] = length_generator[0];\n    multi_scalar_mul_array_return(generators, scalars, true)[0].x\n}\n\n#[field(bn254)]\n#[inline_always]\npub fn derive_generators<let N: u32, let M: u32>(\n    domain_separator_bytes: [u8; M],\n    starting_index: u32,\n) -> [EmbeddedCurvePoint; N] {\n    crate::assert_constant(domain_separator_bytes);\n    // TODO(https://github.com/noir-lang/noir/issues/5672): Add back assert_constant on starting_index\n    __derive_generators(domain_separator_bytes, starting_index)\n}\n\n#[builtin(derive_pedersen_generators)]\n#[field(bn254)]\nfn __derive_generators<let N: u32, let M: u32>(\n    domain_separator_bytes: [u8; M],\n    starting_index: u32,\n) -> [EmbeddedCurvePoint; N] {}\n\n#[field(bn254)]\n// Decompose the input 'bn254 scalar' into two 128 bits limbs.\n// It is called 'unsafe' because it does not assert the limbs are 128 bits\n// Assuming the limbs are 128 bits:\n// Assert the decomposition does not overflow the field size.\nfn from_field_unsafe(scalar: Field) -> EmbeddedCurveScalar {\n    // Safety: xlo and xhi decomposition is checked below\n    let (xlo, xhi) = unsafe { crate::field::bn254::decompose_hint(scalar) };\n    // Check that the decomposition is correct\n    assert_eq(scalar, xlo + crate::field::bn254::TWO_POW_128 * xhi);\n    // Check that the decomposition does not overflow the field size\n    let (a, b) = if xhi == crate::field::bn254::PHI {\n        (xlo, crate::field::bn254::PLO)\n    } else {\n        (xhi, crate::field::bn254::PHI)\n    };\n    crate::field::bn254::assert_lt(a, b);\n\n    EmbeddedCurveScalar { lo: xlo, hi: xhi }\n}\n\npub fn poseidon2_permutation<let N: u32>(input: [Field; N], state_len: u32) -> [Field; N] {\n    assert_eq(input.len(), state_len);\n    poseidon2_permutation_internal(input)\n}\n\n#[foreign(poseidon2_permutation)]\nfn poseidon2_permutation_internal<let N: u32>(input: [Field; N]) -> [Field; N] {}\n\n// Generic hashing support.\n// Partially ported and impacted by rust.\n\n// Hash trait shall be implemented per type.\n#[derive_via(derive_hash)]\npub trait Hash {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher;\n}\n\n// docs:start:derive_hash\ncomptime fn derive_hash(s: TypeDefinition) -> Quoted {\n    let name = quote { $crate::hash::Hash };\n    let signature = quote { fn hash<H>(_self: Self, _state: &mut H) where H: $crate::hash::Hasher };\n    let for_each_field = |name| quote { _self.$name.hash(_state); };\n    crate::meta::make_trait_impl(\n        s,\n        name,\n        signature,\n        for_each_field,\n        quote {},\n        |fields| fields,\n    )\n}\n// docs:end:derive_hash\n\n// Hasher trait shall be implemented by algorithms to provide hash-agnostic means.\n// TODO: consider making the types generic here ([u8], [Field], etc.)\npub trait Hasher {\n    fn finish(self) -> Field;\n\n    fn write(&mut self, input: Field);\n}\n\n// BuildHasher is a factory trait, responsible for production of specific Hasher.\npub trait BuildHasher {\n    type H: Hasher;\n\n    fn build_hasher(self) -> H;\n}\n\npub struct BuildHasherDefault<H>;\n\nimpl<H> BuildHasher for BuildHasherDefault<H>\nwhere\n    H: Hasher + Default,\n{\n    type H = H;\n\n    fn build_hasher(_self: Self) -> H {\n        H::default()\n    }\n}\n\nimpl<H> Default for BuildHasherDefault<H>\nwhere\n    H: Hasher + Default,\n{\n    fn default() -> Self {\n        BuildHasherDefault {}\n    }\n}\n\nimpl Hash for Field {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self);\n    }\n}\n\nimpl Hash for u1 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u8 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u16 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u32 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u64 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u128 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for i8 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u8 as Field);\n    }\n}\n\nimpl Hash for i16 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u16 as Field);\n    }\n}\n\nimpl Hash for i32 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u32 as Field);\n    }\n}\n\nimpl Hash for i64 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u64 as Field);\n    }\n}\n\nimpl Hash for bool {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for () {\n    fn hash<H>(_self: Self, _state: &mut H)\n    where\n        H: Hasher,\n    {}\n}\n\nimpl<T, let N: u32> Hash for [T; N]\nwhere\n    T: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        for elem in self {\n            elem.hash(state);\n        }\n    }\n}\n\nimpl<T> Hash for [T]\nwhere\n    T: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.len().hash(state);\n        for elem in self {\n            elem.hash(state);\n        }\n    }\n}\n\nimpl<A, B> Hash for (A, B)\nwhere\n    A: Hash,\n    B: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n    }\n}\n\nimpl<A, B, C> Hash for (A, B, C)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n    }\n}\n\nimpl<A, B, C, D> Hash for (A, B, C, D)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n    D: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n        self.3.hash(state);\n    }\n}\n\nimpl<A, B, C, D, E> Hash for (A, B, C, D, E)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n    D: Hash,\n    E: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n        self.3.hash(state);\n        self.4.hash(state);\n    }\n}\n\n// Some test vectors for Pedersen hash and Pedersen Commitment.\n// They have been generated using the same functions so the tests are for now useless\n// but they will be useful when we switch to Noir implementation.\n#[test]\nfn assert_pedersen() {\n    assert_eq(\n        pedersen_hash_with_separator([1], 1),\n        0x1b3f4b1a83092a13d8d1a59f7acb62aba15e7002f4440f2275edb99ebbc2305f,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1], 1),\n        EmbeddedCurvePoint {\n            x: 0x054aa86a73cb8a34525e5bbed6e43ba1198e860f5f3950268f71df4591bde402,\n            y: 0x209dcfbf2cfb57f9f6046f44d71ac6faf87254afc7407c04eb621a6287cac126,\n            is_infinite: false,\n        },\n    );\n\n    assert_eq(\n        pedersen_hash_with_separator([1, 2], 2),\n        0x26691c129448e9ace0c66d11f0a16d9014a9e8498ee78f4d69f0083168188255,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2], 2),\n        EmbeddedCurvePoint {\n            x: 0x2e2b3b191e49541fe468ec6877721d445dcaffe41728df0a0eafeb15e87b0753,\n            y: 0x2ff4482400ad3a6228be17a2af33e2bcdf41be04795f9782bd96efe7e24f8778,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3], 3),\n        0x0bc694b7a1f8d10d2d8987d07433f26bd616a2d351bc79a3c540d85b6206dbe4,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3], 3),\n        EmbeddedCurvePoint {\n            x: 0x1fee4e8cf8d2f527caa2684236b07c4b1bad7342c01b0f75e9a877a71827dc85,\n            y: 0x2f9fedb9a090697ab69bf04c8bc15f7385b3e4b68c849c1536e5ae15ff138fd1,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4], 4),\n        0xdae10fb32a8408521803905981a2b300d6a35e40e798743e9322b223a5eddc,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4], 4),\n        EmbeddedCurvePoint {\n            x: 0x07ae3e202811e1fca39c2d81eabe6f79183978e6f12be0d3b8eda095b79bdbc9,\n            y: 0x0afc6f892593db6fbba60f2da558517e279e0ae04f95758587760ba193145014,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5], 5),\n        0xfc375b062c4f4f0150f7100dfb8d9b72a6d28582dd9512390b0497cdad9c22,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5], 5),\n        EmbeddedCurvePoint {\n            x: 0x1754b12bd475a6984a1094b5109eeca9838f4f81ac89c5f0a41dbce53189bb29,\n            y: 0x2da030e3cfcdc7ddad80eaf2599df6692cae0717d4e9f7bfbee8d073d5d278f7,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6], 6),\n        0x1696ed13dc2730062a98ac9d8f9de0661bb98829c7582f699d0273b18c86a572,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6], 6),\n        EmbeddedCurvePoint {\n            x: 0x190f6c0e97ad83e1e28da22a98aae156da083c5a4100e929b77e750d3106a697,\n            y: 0x1f4b60f34ef91221a0b49756fa0705da93311a61af73d37a0c458877706616fb,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7], 7),\n        0x128c0ff144fc66b6cb60eeac8a38e23da52992fc427b92397a7dffd71c45ede3,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7], 7),\n        EmbeddedCurvePoint {\n            x: 0x015441e9d29491b06563fac16fc76abf7a9534c715421d0de85d20dbe2965939,\n            y: 0x1d2575b0276f4e9087e6e07c2cb75aa1baafad127af4be5918ef8a2ef2fea8fc,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8], 8),\n        0x2f960e117482044dfc99d12fece2ef6862fba9242be4846c7c9a3e854325a55c,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8], 8),\n        EmbeddedCurvePoint {\n            x: 0x1657737676968887fceb6dd516382ea13b3a2c557f509811cd86d5d1199bc443,\n            y: 0x1f39f0cb569040105fa1e2f156521e8b8e08261e635a2b210bdc94e8d6d65f77,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9], 9),\n        0x0c96db0790602dcb166cc4699e2d306c479a76926b81c2cb2aaa92d249ec7be7,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9], 9),\n        EmbeddedCurvePoint {\n            x: 0x0a3ceae42d14914a432aa60ec7fded4af7dad7dd4acdbf2908452675ec67e06d,\n            y: 0xfc19761eaaf621ad4aec9a8b2e84a4eceffdba78f60f8b9391b0bd9345a2f2,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 10),\n        0x2cd37505871bc460a62ea1e63c7fe51149df5d0801302cf1cbc48beb8dff7e94,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 10),\n        EmbeddedCurvePoint {\n            x: 0x2fb3f8b3d41ddde007c8c3c62550f9a9380ee546fcc639ffbb3fd30c8d8de30c,\n            y: 0x300783be23c446b11a4c0fabf6c91af148937cea15fcf5fb054abf7f752ee245,\n            is_infinite: false,\n        },\n    );\n}\n","path":"std/hash/mod.nr"},"22":{"source":"pub mod hash;\npub mod aes128;\npub mod array;\npub mod slice;\npub mod ecdsa_secp256k1;\npub mod ecdsa_secp256r1;\npub mod embedded_curve_ops;\npub mod field;\npub mod collections;\npub mod compat;\npub mod convert;\npub mod option;\npub mod string;\npub mod test;\npub mod cmp;\npub mod ops;\npub mod default;\npub mod prelude;\npub mod runtime;\npub mod meta;\npub mod append;\npub mod mem;\npub mod panic;\npub mod hint;\n\nmod primitive_docs;\n\nuse convert::AsPrimitive;\n\n// Oracle calls are required to be wrapped in an unconstrained function\n// Thus, the only argument to the `println` oracle is expected to always be an ident\n#[oracle(print)]\nunconstrained fn print_oracle<T>(with_newline: bool, input: T) {}\n\nunconstrained fn print_unconstrained<T>(with_newline: bool, input: T) {\n    print_oracle(with_newline, input);\n}\n\npub fn println<T>(input: T) {\n    // Safety: a print statement cannot be constrained\n    unsafe {\n        print_unconstrained(true, input);\n    }\n}\n\npub fn print<T>(input: T) {\n    // Safety: a print statement cannot be constrained\n    unsafe {\n        print_unconstrained(false, input);\n    }\n}\n\n/// Asserts the validity of the provided proof and public inputs against the provided verification key and hash.\n///\n/// The ACVM cannot determine whether the provided proof is valid during execution as this requires knowledge of\n/// the backend against which the program is being proven. However if an invalid proof if submitted, the program may\n/// fail to prove or the backend may generate a proof which will subsequently fail to verify.\n///\n/// # Important Note\n///\n/// If you are not developing your own backend such as [Barretenberg](https://github.com/AztecProtocol/barretenberg)\n/// you probably shouldn't need to interact with this function directly. It's easier and safer to use a verification\n/// library which is published by the developers of the backend which will document or enforce any safety requirements.\n///\n/// If you use this directly, you're liable to introduce underconstrainedness bugs and *your circuit will be insecure*.\n///\n/// # Arguments\n/// - verification_key: The verification key of the circuit to be verified.\n/// - proof: The proof to be verified.\n/// - public_inputs: The public inputs associated with `proof`\n/// - key_hash: The hash of `verification_key` of the form expected by the backend.\n/// - proof_type: An identifier for the proving scheme used to generate the proof to be verified. This allows\n///               for a single backend to support verifying multiple proving schemes.\n///\n/// # Constraining `key_hash`\n///\n/// The Noir compiler does not by itself constrain that `key_hash` is a valid hash of `verification_key`.\n/// This is because different backends may differ in how they hash their verification keys.\n/// It is then the responsibility of either the noir developer (by explicitly hashing the verification key\n/// in the correct manner) or by the proving system itself internally asserting the correctness of `key_hash`.\npub fn verify_proof_with_type<let N: u32, let M: u32, let K: u32>(\n    verification_key: [Field; N],\n    proof: [Field; M],\n    public_inputs: [Field; K],\n    key_hash: Field,\n    proof_type: u32,\n) {\n    if !crate::runtime::is_unconstrained() {\n        crate::assert_constant(proof_type);\n    }\n    verify_proof_internal(verification_key, proof, public_inputs, key_hash, proof_type);\n}\n\n#[foreign(recursive_aggregation)]\nfn verify_proof_internal<let N: u32, let M: u32, let K: u32>(\n    verification_key: [Field; N],\n    proof: [Field; M],\n    public_inputs: [Field; K],\n    key_hash: Field,\n    proof_type: u32,\n) {}\n\n// Asserts that the given value is known at compile-time.\n// Useful for debugging for-loop bounds.\n#[builtin(assert_constant)]\npub fn assert_constant<T>(x: T) {}\n\n// Asserts that the given value is both true and known at compile-time.\n// The message can be a string, a format string, or any value, as long as it is known at compile-time\n#[builtin(static_assert)]\npub fn static_assert<T>(predicate: bool, message: T) {}\n\n#[deprecated(\"wrapping operations should be done with the Wrapping traits. E.g: x.wrapping_add(y)\")]\npub fn wrapping_add<T>(x: T, y: T) -> T\nwhere\n    T: AsPrimitive<Field>,\n    Field: AsPrimitive<T>,\n{\n    AsPrimitive::as_(x.as_() + y.as_())\n}\n#[deprecated(\"wrapping operations should be done with the Wrapping traits. E.g: x.wrapping_sub(y)\")]\npub fn wrapping_sub<T>(x: T, y: T) -> T\nwhere\n    T: AsPrimitive<Field>,\n    Field: AsPrimitive<T>,\n{\n    //340282366920938463463374607431768211456 is 2^128, it is used to avoid underflow\n    AsPrimitive::as_(x.as_() + 340282366920938463463374607431768211456 - y.as_())\n}\n#[deprecated(\"wrapping operations should be done with the Wrapping traits. E.g: x.wrapping_mul(y)\")]\npub fn wrapping_mul<T>(x: T, y: T) -> T\nwhere\n    T: AsPrimitive<Field>,\n    Field: AsPrimitive<T>,\n{\n    AsPrimitive::as_(x.as_() * y.as_())\n}\n\n#[builtin(as_witness)]\npub fn as_witness(x: Field) {}\n","path":"std/lib.nr"},"51":{"source":"// SPDX-License-Identifier: LGPL-3.0-only\n//\n// This file is provided WITHOUT ANY WARRANTY;\n// without even the implied warranty of MERCHANTABILITY\n// or FITNESS FOR A PARTICULAR PURPOSE.\n\nuse bb_proof_verification::{UltraHonkProof, UltraHonkVerificationKey, verify_honk_proof_non_zk};\nuse lib::math::commitments::compute_recursive_aggregation_commitment;\n\nfn main(\n    verification_key: UltraHonkVerificationKey,\n    proofs: [UltraHonkProof; 2],\n    commitments: pub [Field; 2],\n    key_hash: Field,\n) -> pub Field {\n    verify_honk_proof_non_zk(verification_key, proofs[0], [commitments[0]], key_hash);\n    verify_honk_proof_non_zk(verification_key, proofs[1], [commitments[1]], key_hash);\n\n    let mut commitments_vec = Vec::new();\n\n    commitments_vec.push(commitments[0]);\n    commitments_vec.push(commitments[1]);\n\n    compute_recursive_aggregation_commitment(commitments_vec)\n}\n","path":"enclave/circuits/bin/recursive_aggregation/fold/src/main.nr"},"52":{"source":"// Constants for UltraHonk recursive verifier inputs\npub global PROOF_TYPE_HONK: u32 = 0; // identifier for UltraHonk verfier\npub global RECURSIVE_PROOF_LENGTH: u32 = 457;\npub global ULTRA_VK_LENGTH_IN_FIELDS: u32 = 115;\n\npub type UltraHonkProof = [Field; RECURSIVE_PROOF_LENGTH];\npub type UltraHonkVerificationKey = [Field; ULTRA_VK_LENGTH_IN_FIELDS];\n\n// Constants for Rollup-UltraHonk recursive verifier inputs (N.B. this is equivalent to UH plus IPA claim and proof)\npub global PROOF_TYPE_ROLLUP_HONK: u32 = 4; // identifier for rollup-UltraHonk verfier\npub global IPA_CLAIM_SIZE: u32 = 10;\npub global IPA_PROOF_LENGTH: u32 = 64;\npub global RECURSIVE_ROLLUP_HONK_PROOF_LENGTH: u32 =\n    RECURSIVE_PROOF_LENGTH + IPA_CLAIM_SIZE + IPA_PROOF_LENGTH;\n\npub type RollupHonkProof = [Field; RECURSIVE_ROLLUP_HONK_PROOF_LENGTH];\npub type RollupHonkVerificationKey = [Field; ULTRA_VK_LENGTH_IN_FIELDS];\n\npub global PROOF_TYPE_HONK_ZK: u32 = 6; // identifier for UltraHonk ZK verfier\npub global RECURSIVE_ZK_PROOF_LENGTH: u32 = 492 + 16;\n\npub type UltraHonkZKProof = [Field; RECURSIVE_ZK_PROOF_LENGTH];\n\n// Verifies a non-zero-knowledge UltraHonk proof.\n//\n// Represents standard UltraHonk recursive verification for proofs that do not hide the witness.\n// Use this only in situations where zero-knowledge is not required.\npub fn verify_honk_proof_non_zk<let N: u32>(\n    verification_key: UltraHonkVerificationKey,\n    proof: UltraHonkProof,\n    public_inputs: [Field; N],\n    key_hash: Field, // Hash of the verification key\n) {\n    std::verify_proof_with_type(\n        verification_key,\n        proof,\n        public_inputs,\n        key_hash,\n        PROOF_TYPE_HONK,\n    );\n}\n\n// Verifies a non-zero-knowledge Rollup UltraHonk proof with IPA (Inner Product Argument).\n//\n// This variant includes an IPA claim and proof appended to the standard UltraHonk proof,\n// used to amortize IPA recursive verification costs in rollup circuits.\npub fn verify_rolluphonk_proof<let N: u32>(\n    verification_key: RollupHonkVerificationKey,\n    proof: RollupHonkProof,\n    public_inputs: [Field; N],\n    key_hash: Field, // Hash of the verification key\n) {\n    std::verify_proof_with_type(\n        verification_key,\n        proof,\n        public_inputs,\n        key_hash,\n        PROOF_TYPE_ROLLUP_HONK,\n    );\n}\n\n// Verifies a zero-knowledge UltraHonk proof.\n//\n// This verifier is for UltraHonk proofs constructed with zero-knowledge, which hide the witness\n// values from the verifier.\n// Note: We intentionally choose the generic name \"verify_honk_proof\" for this function, as we \n// want ZK to be the default unless the user explicitly opts out.\npub fn verify_honk_proof<let N: u32>(\n    verification_key: UltraHonkVerificationKey,\n    proof: UltraHonkZKProof,\n    public_inputs: [Field; N],\n    key_hash: Field, // Hash of the verification key\n) {\n    std::verify_proof_with_type(\n        verification_key,\n        proof,\n        public_inputs,\n        key_hash,\n        PROOF_TYPE_HONK_ZK,\n    );\n}\n","path":"/Users/omardesogus/nargo/github.com/AztecProtocol/aztec-packages/v3.0.0-nightly.20260102/barretenberg/noir/bb_proof_verification/src/lib.nr"},"77":{"source":"// SPDX-License-Identifier: LGPL-3.0-only\n//\n// This file is provided WITHOUT ANY WARRANTY;\n// without even the implied warranty of MERCHANTABILITY\n// or FITNESS FOR A PARTICULAR PURPOSE.\n\nuse crate::math::helpers::{compute_safe, flatten};\nuse crate::math::polynomial::Polynomial;\n\n/// DOMAIN SEPARATORS\n\n// Domain separator - \"PK\"\npub global DS_PK: [u8; 64] = [\n    0x50, 0x4b, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"PK_GENERATION\"\npub global DS_PK_GENERATION: [u8; 64] = [\n    0x50, 0x4b, 0x5f, 0x47, 0x45, 0x4e, 0x45, 0x52, 0x41, 0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"SHARE_COMPUTATION\"\npub global DS_SHARE_COMPUTATION: [u8; 64] = [\n    0x53, 0x48, 0x41, 0x52, 0x45, 0x5f, 0x43, 0x4f, 0x4d, 0x50, 0x55, 0x54, 0x41, 0x54, 0x49, 0x4f,\n    0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"SHARE_ENCRYPTION\"\npub global DS_SHARE_ENCRYPTION: [u8; 64] = [\n    0x53, 0x48, 0x41, 0x52, 0x45, 0x5f, 0x45, 0x4e, 0x43, 0x52, 0x59, 0x50, 0x54, 0x49, 0x4f, 0x4e,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"PK_AGGREGATION\"\npub global DS_PK_AGGREGATION: [u8; 64] = [\n    0x50, 0x4b, 0x5f, 0x41, 0x47, 0x47, 0x52, 0x45, 0x47, 0x41, 0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"CIPHERTEXT\"\npub global DS_CIPHERTEXT: [u8; 64] = [\n    0x43, 0x49, 0x50, 0x48, 0x45, 0x52, 0x54, 0x45, 0x58, 0x54, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"AGGREGATED_SHARES\"\npub global DS_AGGREGATED_SHARES: [u8; 64] = [\n    0x41, 0x47, 0x47, 0x52, 0x45, 0x47, 0x41, 0x54, 0x45, 0x44, 0x5f, 0x53, 0x48, 0x41, 0x52, 0x45,\n    0x53, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"RECURSIVE_AGGREGATION\"\npub global DS_RECURSIVE_AGGREGATION: [u8; 64] = [\n    0x52, 0x45, 0x43, 0x55, 0x52, 0x53, 0x49, 0x56, 0x45, 0x5f, 0x41, 0x47, 0x47, 0x52, 0x45, 0x47,\n    0x41, 0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"CLG_PK_GENERATION\"\npub global DS_CLG_PK_GENERATION: [u8; 64] = [\n    0x43, 0x4c, 0x47, 0x5f, 0x50, 0x4b, 0x5f, 0x47, 0x45, 0x4e, 0x45, 0x52, 0x41, 0x54, 0x49, 0x4f,\n    0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"CLG_SHARE_ENCRYPTION\"\npub global DS_CLG_SHARE_ENCRYPTION: [u8; 64] = [\n    0x43, 0x4c, 0x47, 0x5f, 0x53, 0x48, 0x41, 0x52, 0x45, 0x5f, 0x45, 0x4e, 0x43, 0x52, 0x59, 0x50,\n    0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"CLG_USER_DATA_ENCRYPTION\"\npub global DS_CLG_USER_DATA_ENCRYPTION: [u8; 64] = [\n    0x43, 0x4c, 0x47, 0x5f, 0x55, 0x53, 0x45, 0x52, 0x5f, 0x44, 0x41, 0x54, 0x41, 0x5f, 0x45, 0x4e,\n    0x43, 0x52, 0x59, 0x50, 0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n// Domain separator - \"CLG_SHARE_DECRYPTION\"\npub global DS_CLG_SHARE_DECRYPTION: [u8; 64] = [\n    0x43, 0x4c, 0x47, 0x5f, 0x53, 0x48, 0x41, 0x52, 0x45, 0x5f, 0x44, 0x45, 0x43, 0x52, 0x59, 0x50,\n    0x54, 0x49, 0x4f, 0x4e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n\n// Domain separator - \"USER_DATA_ENCRYPTION_COMMITMENT\"\npub global DS_USER_DATA_ENCRYPTION_COMMITMENT: [u8; 64] = [\n    0x55, 0x53, 0x45, 0x52, 0x5f, 0x44, 0x41, 0x54, 0x41, 0x5f, 0x45, 0x4e, 0x43, 0x52, 0x59, 0x50,\n    0x54, 0x49, 0x4f, 0x4e, 0x5f, 0x43, 0x4f, 0x4d, 0x4d, 0x49, 0x54, 0x4d, 0x45, 0x4e, 0x54, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n\n/// WRAPPERS\n\npub fn compute_commitment(inputs: Vec<Field>, domain_separator: [u8; 64]) -> Field {\n    compute_safe(domain_separator, inputs, [0x80000000 | inputs.len(), 1]).get(0)\n}\n\npub fn compute_single_polynomial_commitment<let N: u32, let BIT: u32>(\n    polynomial: Polynomial<N>,\n    domain_separator: [u8; 64],\n) -> Field {\n    let mut payload = single_polynomial_payload::<N, BIT>(Vec::new(), polynomial);\n    compute_commitment(payload, domain_separator)\n}\n\npub fn compute_multiple_polynomial_commitment<let N: u32, let L: u32, let BIT: u32>(\n    polynomials: [Polynomial<N>; L],\n    domain_separator: [u8; 64],\n) -> Field {\n    let mut payload = multiple_polynomial_payload::<N, L, BIT>(Vec::new(), polynomials);\n    compute_commitment(payload, domain_separator)\n}\n\npub fn compute_challenge<let L: u32>(inputs: Vec<Field>, domain_separator: [u8; 64]) -> Vec<Field> {\n    compute_safe(domain_separator, inputs, [0x80000000 | inputs.len(), 2 * L])\n}\n\npub fn single_polynomial_payload<let N: u32, let BIT_POLY: u32>(\n    payload: Vec<Field>,\n    input: Polynomial<N>,\n) -> Vec<Field> {\n    flatten::<_, _, BIT_POLY>(payload, [input])\n}\n\npub fn multiple_polynomial_payload<let N: u32, let L: u32, let BIT_POLY: u32>(\n    payload: Vec<Field>,\n    inputs: [Polynomial<N>; L],\n) -> Vec<Field> {\n    flatten::<_, _, BIT_POLY>(payload, inputs)\n}\n\n/// COMMITMENTS\n\npub fn compute_dkg_pk_commitment<let N: u32, let L: u32, let BIT_PK: u32>(\n    pk0: [Polynomial<N>; L],\n    pk1: [Polynomial<N>; L],\n) -> Field {\n    let mut payload = multiple_polynomial_payload::<N, L, BIT_PK>(Vec::new(), pk0);\n    payload = multiple_polynomial_payload::<N, L, BIT_PK>(payload, pk1);\n\n    compute_commitment(payload, DS_PK)\n}\n\npub fn compute_threshold_pk_commitment<let N: u32, let L: u32, let BIT_PK: u32>(\n    pk0: [Polynomial<N>; L],\n    pk1: [Polynomial<N>; L],\n) -> Field {\n    let mut payload = multiple_polynomial_payload::<N, L, BIT_PK>(Vec::new(), pk0);\n    payload = multiple_polynomial_payload::<N, L, BIT_PK>(payload, pk1);\n\n    compute_commitment(payload, DS_PK_GENERATION)\n}\n\npub fn compute_share_computation_sk_commitment<let N: u32, let BIT_SK: u32>(\n    sk: Polynomial<N>,\n) -> Field {\n    let mut payload = single_polynomial_payload::<N, BIT_SK>(Vec::new(), sk);\n    compute_commitment(payload, DS_SHARE_COMPUTATION)\n}\n\npub fn compute_share_computation_e_sm_commitment<let N: u32, let L: u32, let BIT_E_SM: u32>(\n    e_sm: [Polynomial<N>; L],\n) -> Field {\n    let mut payload = multiple_polynomial_payload::<N, L, BIT_E_SM>(Vec::new(), e_sm);\n    compute_commitment(payload, DS_SHARE_COMPUTATION)\n}\n\npub fn compute_share_encryption_commitment_from_message<let N: u32, let BIT_MSG: u32>(\n    message: Polynomial<N>,\n) -> Field {\n    let mut payload = single_polynomial_payload::<N, BIT_MSG>(Vec::new(), message);\n    compute_commitment(payload, DS_SHARE_ENCRYPTION)\n}\n\npub fn compute_share_encryption_commitment_from_shares<let N: u32, let L: u32, let N_PARTIES: u32>(\n    y: [[[Field; N_PARTIES + 1]; L]; N],\n    party_idx: u32,\n    mod_idx: u32,\n) -> Field {\n    let mut payload = Vec::new();\n\n    for coeff_idx in 0..N {\n        payload.push(y[coeff_idx][mod_idx][party_idx + 1]);\n    }\n\n    // Include party_idx and mod_idx in the hash\n    payload.push(party_idx as Field);\n    payload.push(mod_idx as Field);\n\n    compute_commitment(payload, DS_SHARE_ENCRYPTION)\n}\n\npub fn compute_aggregated_shares_commitment<let N: u32, let L: u32, let BIT_MSG: u32>(\n    agg_shares: [Polynomial<N>; L],\n) -> Field {\n    let mut payload = multiple_polynomial_payload::<N, L, BIT_MSG>(Vec::new(), agg_shares);\n    compute_commitment(payload, DS_AGGREGATED_SHARES)\n}\n\npub fn compute_pk_aggregation_commitment<let N: u32, let L: u32, let BIT_PK: u32>(\n    pk0: [Polynomial<N>; L],\n    pk1: [Polynomial<N>; L],\n) -> Field {\n    let commit_pk0 = compute_multiple_polynomial_commitment::<N, L, BIT_PK>(pk0, DS_PK_AGGREGATION);\n    let commit_pk1 = compute_multiple_polynomial_commitment::<N, L, BIT_PK>(pk1, DS_PK_AGGREGATION);\n\n    let mut inputs = Vec::new();\n    inputs.push(commit_pk0);\n    inputs.push(commit_pk1);\n\n    compute_commitment(inputs, DS_PK_AGGREGATION)\n}\n\npub fn compute_recursive_aggregation_commitment(payload: Vec<Field>) -> Field {\n    compute_commitment(payload, DS_RECURSIVE_AGGREGATION)\n}\n\npub fn compute_ciphertext_commitment<let N: u32, let L: u32, let BIT_CT: u32>(\n    ct0: [Polynomial<N>; L],\n    ct1: [Polynomial<N>; L],\n) -> Field {\n    let commit_ct0 = compute_multiple_polynomial_commitment::<N, L, BIT_CT>(ct0, DS_CIPHERTEXT);\n    let commit_ct1 = compute_multiple_polynomial_commitment::<N, L, BIT_CT>(ct1, DS_CIPHERTEXT);\n\n    let mut inputs = Vec::new();\n    inputs.push(commit_ct0);\n    inputs.push(commit_ct1);\n\n    compute_commitment(inputs, DS_CIPHERTEXT)\n}\n\n/// COMMITMENTS FOR CHALLENGES\n\npub fn compute_threshold_pk_challenge<let L: u32>(payload: Vec<Field>) -> Vec<Field> {\n    compute_challenge::<L>(payload, DS_CLG_PK_GENERATION)\n}\n\npub fn compute_share_encryption_challenge<let L: u32>(payload: Vec<Field>) -> Vec<Field> {\n    compute_challenge::<L>(payload, DS_CLG_SHARE_ENCRYPTION)\n}\n\npub fn compute_threshold_share_decryption_challenge<let L: u32>(payload: Vec<Field>) -> Field {\n    compute_challenge::<L>(payload, DS_CLG_SHARE_DECRYPTION).get(0)\n}\n\npub fn compute_user_data_encryption_ct0_challenge<let L: u32>(payload: Vec<Field>) -> Vec<Field> {\n    compute_challenge::<L>(payload, DS_CLG_USER_DATA_ENCRYPTION)\n}\n\npub fn compute_user_data_encryption_ct1_challenge<let L: u32>(payload: Vec<Field>) -> Vec<Field> {\n    compute_challenge::<L>(payload, DS_CLG_USER_DATA_ENCRYPTION)\n}\n","path":"enclave/circuits/lib/src/math/commitments.nr"},"78":{"source":"// SPDX-License-Identifier: LGPL-3.0-only\n//\n// This file is provided WITHOUT ANY WARRANTY;\n// without even the implied warranty of MERCHANTABILITY\n// or FITNESS FOR A PARTICULAR PURPOSE.\n\n//! Helper functions for circuit construction and cryptographic operations.\nuse crate::math::polynomial::Polynomial;\nuse crate::math::safe::SafeSponge;\n\n/// Compute hex-aligned packing parameters for a given `BIT`.\n///\n/// # Purpose\n/// Returns `(nibble_bits, group)` for use by pack/flatten so layout stays consistent.\n/// - `nibble_bits`: ceil (`BIT`) to the next multiple of 4 (nibble alignment).\n///   - Examples: `BIT = 7 -> 8`, `BIT = 8 -> 8`, `BIT = 9 -> 12`, `BIT = 10 -> 12`, `BIT = 11 -> 12`,\n///     `BIT=16 -> 16`, `BIT = 17 -> 20`.\n/// - `group`: max number of encoded limbs that fit in one BN254 field element,\n///            when each limb uses an extra 4 bits (see below).\n///\n/// # Rationale\n/// - We align to nibbles so powers of two are hex-friendly and deterministic.\n/// - We reserve one extra nibble (4 bits) per stored value to lift signed\n///   coefficients into the non-negative range (e.g., store `v + 2^nibble_bits`),\n///   which implies a radix of `2^(nibble_bits + 4)`.\n///\n/// # Safety\n/// - Asserts `nibble_bits + 4 <= 254` to avoid mod-p wrap on BN254.\n/// - Ensures at least one limb fits: `group >= 1`.\nfn packing_layout<let BIT: u32>() -> (u32, u32) {\n    // Ceil BIT up to the next multiple of 4 (nibble alignment).\n    let nibble_bits = ((BIT + 3) / 4) * 4;\n\n    // Each stored limb uses an extra nibble because negative coefficients\n    // will be shifted to positive, so radix = 2^(nibble_bits+4).\n    assert(nibble_bits + 4 <= 254);\n\n    // Maximum limbs that fit in one BN254 element without wrap.\n    let group = 254 / (nibble_bits + 4);\n    assert(group >= 1);\n    (nibble_bits, group)\n}\n\n/// Flatten `L` polynomials into a single linear stream of packed `Field` carriers.\n///\n/// ## What this does\n/// - For each CRT limb `j` in `0..L`, it packs the coefficients of `poly[j]`\n///   with `pack::<A, BIT>` and appends all resulting carriers to `inputs`.\n/// - The packing layout (nibble-aligned width and `group` size) is taken from\n///   `packing_layout::<BIT>()` and must match what `pack` uses.\n///\n/// ## Determinism & order\n/// - Preserves a stable order: iterate `j = 0..L`, then for each `j` append\n///   carriers in ascending chunk index `i = 0..num_chunks`.\n/// - This ensures transcripts remain deterministic across runs.\n///\n/// ## Generics\n/// - `A`: polynomial degree (number of coefficients per polynomial).\n/// - `L`: number of CRT bases (polynomials).\n/// - `BIT`: per-coefficient bit bound used by the packing layout (compile-time).\n///\n/// ## Returns\n/// - The same `inputs` vector, extended with all carriers in deterministic order.\npub fn flatten<let A: u32, let L: u32, let BIT: u32>(\n    mut inputs: Vec<Field>,\n    poly: [Polynomial<A>; L],\n) -> Vec<Field> {\n    for j in 0..L {\n        // Pack its A coefficients into `num_chunks` carriers using the same BIT layout.\n        let packed = pack::<A, BIT>(poly[j].coefficients);\n\n        // Append carriers in-order to `inputs` to keep a stable transcript layout.\n        for i in 0..packed.len() {\n            inputs.push(packed.get(i));\n        }\n    }\n\n    // Return the extended input stream.\n    inputs\n}\n\n/// Pack `A` values into a `Vec<Field>` of carriers using the shared hex-aligned layout.\n///\n/// ## What this does\n/// - Computes `(nibble_bits, group)` via `packing_layout::<BIT>()`.\n/// - Encodes each value as a limb `digit = v + 2^nibble_bits` and concatenates\n///   limbs in base `radix = 2^(nibble_bits + 4)` (one extra nibble of headroom).\n/// - Packs up to `group` limbs per carrier (fits within BN254 254-bit capacity).\n/// - Pads the last, partial carrier with `digit = 2^nibble_bits` to keep a stable layout.\n///\n/// ## Determinism & order\n/// - Processes values in increasing index order and emits carriers in chunk order\n///   (`chunk = 0..num_chunks`). Padding is deterministic.\n///\n/// ## Generics\n/// - `A`: number of input values.\n/// - `BIT`: per-value bit bound; rounded up to `nibble_bits` by `packing_layout`.\n///\n/// ## Preconditions / Notes\n/// - Call with the raw coefficients whose magnitudes already satisfy the BIT bound\n///   (as enforced by the upstream range checks); `pack` performs the signed -> unsigned\n///   shift internally via `v + base`.\n/// - `group >= 1` is enforced by `packing_layout::<BIT>()`.\n/// - Padding with `digit = 2^nibble_bits` encodes `zero limb` consistently.\n///\n/// ## Returns\n/// - A `Vec<Field>` where each element is a concatenation of up to `group` limbs,\n///   suitable for hashing or transcript I/O.\npub fn pack<let A: u32, let BIT: u32>(values: [Field; A]) -> Vec<Field> {\n    // Layout parameters: nibble-aligned width and limbs-per-carrier group size.\n    let (nibble_bits, group) = packing_layout::<BIT>();\n\n    let base = 2.pow_32(nibble_bits as Field); // 2^nibble_bits\n    let radix = 2.pow_32((nibble_bits + 4) as Field); // 2^(nibble_bits + 4)\n\n    // Number of chunks to emit: ceil(A / group).\n    let num_chunks = (A + group - 1) / group;\n    let mut out = Vec::new();\n\n    // Process in fixed-size chunks of `group` limbs.\n    for chunk in 0..num_chunks {\n        // How many real values go into this chunk.\n        let remain = A - (chunk * group);\n        let take = if remain < group { remain } else { group };\n\n        // Build field element accumulator (big-endian concatenation in `radix`).\n        let mut acc = 0;\n        for i in 0..take {\n            let v = values[chunk * group + i];\n            acc = acc * radix + (v + base);\n        }\n\n        // Pad remaining limb slots with the canonical zero-limb `digit = base`.\n        for _ in 0..(group - take) {\n            acc = acc * radix + base;\n        }\n\n        out.push(acc);\n    }\n    out\n}\n\n/// Computes a cryptographic hash using the SAFE (Sponge API for Field Elements) protocol.\n///\n/// This is a convenience wrapper around the SAFE sponge API that handles the full\n/// lifecycle: initialization, absorption, squeezing, and finalization. It's designed\n/// for use in Fiat-Shamir challenge generation and commitment schemes within zero-knowledge circuits.\n///\n/// # Arguments\n/// * `domain_separator` - A 64-byte domain separator used to differentiate between\n///                        different protocol instances and prevent cross-protocol attacks.\n/// * `inputs` - Vector of field elements to be absorbed into the sponge.\n/// * `io_pattern` - A 2-element array encoding the I/O pattern:\n///                 - `io_pattern[0]`: Encoded ABSORB operation (MSB=1, lower 31 bits = length)\n///                 - `io_pattern[1]`: Encoded SQUEEZE operation (MSB=0, lower 31 bits = length)\n///\n/// # Returns\n/// A vector of field elements squeezed from the sponge, with length determined by\n/// the SQUEEZE operation in the IO pattern.\npub fn compute_safe(\n    domain_separator: [u8; 64],\n    inputs: Vec<Field>,\n    io_pattern: [u32; 2],\n) -> Vec<Field> {\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(inputs);\n    let digests = sponge.squeeze();\n    sponge.finish();\n\n    digests\n}\n\n#[test]\nfn test_flatten() {\n    // Create test polynomials\n    let poly1 = Polynomial::new([1, 2, 3]); // degree 2\n    let poly2 = Polynomial::new([4, -16, 6]); // degree 2\n    let poly3 = Polynomial::new([-7, 8, 9]); // degree 2\n\n    let polynomials = [poly1, poly2, poly3];\n\n    // Initialize target array with zeros\n    let mut inputs = Vec::new();\n\n    // Flatten the polynomials\n    let result = flatten::<_, _, 4>(inputs, polynomials);\n\n    // Verify the flattened coefficients are in the correct positions\n    // Every value shifted 1 nibble incase of negative integers\n    assert(result.get(0) == 0x11121310101010101010101010101010101010101010101010101010101010);\n    assert(result.get(1) == 0x14001610101010101010101010101010101010101010101010101010101010); // -16 became 00 at  0x 14 00 16,\n    assert(result.get(2) == 0x09181910101010101010101010101010101010101010101010101010101010); // -7 became 09 at 0x 09 18 19(16 - 7 = 9)\n}\n\n#[test]\nfn test_flatten_big() {\n    // Create test polynomials\n    let poly1 = Polynomial::new([\n        1791218451968394,\n        21888242871839275222246405745257275088548364400416034343698198265248580087864,\n        21888242871839275222246405745257275088548364400416034343698200542108324633466,\n        5430119342984413,\n        704811298945172,\n        8901715723925099,\n        21888242871839275222246405745257275088548364400416034343698203098124042812559,\n        21888242871839275222246405745257275088548364400416034343698200215091693880034,\n    ]);\n    let poly2 = Polynomial::new([\n        21888242871839275222246405745257275088548364400416034343698200314078269634250,\n        21888242871839275222246405745257275088548364400416034343698200967285641915872,\n        2909990636858607,\n        7896103832076587,\n        2078397209533893,\n        21888242871839275222246405745257275088548364400416034343698199792421452734531,\n        614400389245817,\n        8290314119277588,\n    ]);\n    let poly3 = Polynomial::new([\n        21888242871839275222246405745257275088548364400416034343698201373175279892906,\n        21888242871839275222246405745257275088548364400416034343698201087241869723721,\n        6768789983786188,\n        635797784303388,\n        7610153424227556,\n        4633893206538324,\n        2016269760615332,\n        21888242871839275222246405745257275088548364400416034343698201007080554428142,\n    ]);\n\n    let polynomials = [poly1, poly2, poly3];\n\n    // Initialize target array with zeros\n    let mut inputs = Vec::new();\n\n    // Flatten the polynomials\n    let result = flatten::<_, _, 54>(inputs, polynomials);\n\n    // Verify the flattened coefficients are in the correct positions\n    // Every value shifted 1 nibble incase of negative integers\n\n    // For the first index of result operation goes like this,\n\n    // First four index of poly1\n    // 1791218451968394,\n    // 21888242871839275222246405745257275088548364400416034343698198265248580087864,\n    // 21888242871839275222246405745257275088548364400416034343698200542108324633466,\n    // 5430119342984413,\n\n    // base + 1791218451968394 = 0x1065d1a8b8b718a\n    // base - 5921327228407753 = 0xeaf69591f3b037 (negative coefficient shifted)\n    // base - 3644467483862151 = 0xf30d604a3a9b79 (negative coefficient shifted)\n    // base + 5430119342984413 = 0x1134aaa2e86ccdd\n    assert(result.get(0) == 0x1065d1a8b8b718a0eaf69591f3b0370f30d604a3a9b791134aaa2e86ccdd);\n    assert(result.get(1) == 0x1028105ab1b789411fa010339db66b0fc220f1326bc8e0f1e3f4cc1e02e1);\n    assert(result.get(2) == 0x0f23dfbe7cd76c90f4901299312ddf10a569efe35acef11c0d76f005412b);\n    assert(result.get(3) == 0x107624a8f605dc50f0638a368960421022ecb3cf36b7911d73ff2c27ec14);\n    assert(result.get(4) == 0x0f6013a24e1b9a90f4fd2c158a08481180c2dba8af4cc10242413515171c);\n    assert(result.get(5) == 0x11b0964eb898ce411076805680b85410729c962da53a40f4b44412d0f6ed);\n}\n\n#[test]\nfn test_flatten_small() {\n    // Create test polynomials\n    let poly1 = Polynomial::new([712345, 104857, 999999, 500001, 123, 654321, 77]);\n    let poly2 = Polynomial::new([1, 524287, 888888, 23456, 34567, 765432, 0]);\n    let poly3 = Polynomial::new([444444, 333333, 222222, 111111, 987654, 246810, 13579]);\n\n    let polynomials = [poly1, poly2, poly3];\n\n    // Initialize target array with zeros\n    let mut inputs = Vec::new();\n\n    // Flatten the polynomials\n    let result = flatten::<_, _, 20>(inputs, polynomials);\n\n    assert(result.get(0) == 0x1ade991199991f423f17a12110007b19fbf110004d100000100000100000);\n    assert(result.get(1) == 0x10000117ffff1d9038105ba01087071badf8100000100000100000100000);\n    assert(result.get(2) == 0x16c81c15161513640e11b2071f120613c41a10350b100000100000100000);\n}\n\n#[test]\nfn test_safe_hashing_with_safe_helper() {\n    // Verifies basic hash functionality with a simple ABSORB(3) + SQUEEZE(1) pattern.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let elements = Vec::from_slice(&[1, 2, 3]);\n\n    // Pattern: ABSORB(3), SQUEEZE(1)\n    let io_pattern = [0x80000003, 0x00000001];\n    let digests1 = compute_safe(domain_separator, elements, io_pattern);\n\n    assert(digests1.len() == 1);\n    assert(digests1.get(0) != 0);\n\n    // Test determinism\n    let digests2 = compute_safe(domain_separator, elements, io_pattern);\n\n    assert(digests2.len() == 1);\n    assert(digests2.get(0) != 0);\n    assert(digests2.get(0) == digests1.get(0));\n}\n\n#[test]\nfn test_pack() {\n    // Test pack function directly with small values\n    let values = [1, 2, 3, 4];\n    let packed = pack::<4, 4>(values);\n\n    // With BIT=4, nibble_bits=4, group should be floor(254/(4+4)) = 31\n    // So all 4 values should fit in one carrier\n    assert(packed.len() >= 1);\n\n    // Test with negative values\n    let values_neg = [-1, 2, -3, 4];\n    let packed_neg = pack::<4, 4>(values_neg);\n    assert(packed_neg.len() >= 1);\n}\n\n#[test]\nfn test_pack_single_value() {\n    // Test packing a single value\n    let values = [42];\n    let packed = pack::<1, 8>(values);\n    assert(packed.len() == 1);\n    assert(packed.get(0) != 0);\n}\n\n#[test]\nfn test_pack_determinism() {\n    // Test that packing is deterministic\n    let values = [10, 20, 30];\n    let packed1 = pack::<3, 8>(values);\n    let packed2 = pack::<3, 8>(values);\n\n    assert(packed1.len() == packed2.len());\n    for i in 0..packed1.len() {\n        assert(packed1.get(i) == packed2.get(i));\n    }\n}\n","path":"enclave/circuits/lib/src/math/helpers.nr"},"84":{"source":"// SPDX-License-Identifier: LGPL-3.0-only\n//\n// This file is provided WITHOUT ANY WARRANTY;\n// without even the implied warranty of MERCHANTABILITY\n// or FITNESS FOR A PARTICULAR PURPOSE.\n\nuse keccak256::keccak256;\nuse poseidon::poseidon2_permutation;\n\n/// SAFE (Sponge API for Field Elements)\n///\n/// This module provides a complete implementation of the SAFE API in Noir as defined in:\n/// \"SAFE (Sponge API for Field Elements) - A Toolbox for ZK Hash Applications\"\n/// see https://hackmd.io/bHgsH6mMStCVibM_wYvb2w#22-Sponge-state for more details.\n///\n/// SAFE provides a unified interface for cryptographic sponge functions that can be\n/// instantiated with various permutations to create hash functions, MACs, authenticated\n/// encryption schemes, and other cryptographic primitives for ZK proof systems.\n///\n/// This implementation follows the SAFE specification exactly, providing:\n/// - Complete API: START, ABSORB, SQUEEZE, FINISH operations.\n/// - Full security: Domain separation, tag computation, IO pattern validation.\n/// - Poseidon2 integration: Field-friendly permutation for ZK systems.\n/// - Specification compliance: All operations follow SAFE spec 2.4 exactly.\n/// - Natural API design: Variable-length inputs, automatic length detection from IO patterns.\n///\n/// # API Design\n///\n/// The API is designed for natural usage while maintaining type safety:\n/// - `absorb(input: [Field])`: Accepts variable-length arrays, no padding required.\n/// - `squeeze()`: Returns a vector with field element(s).\n/// - IO patterns automatically determine operation lengths for validation.\n\n/// Rate parameter for the sponge construction (number of field elements that can be absorbed per permutation call).\nglobal RATE: u32 = 3;\n\n/// Capacity parameter for the sponge construction (security parameter, typically 1-2 field elements).\nglobal CAPACITY: u32 = 1;\n\n/// Total state size (rate + capacity) in field elements.\nglobal STATE_SIZE: u32 = RATE + CAPACITY;\n\n/// IO Pattern encoding constants (from SAFE spec 2.3).\n///\n/// These constants are used for encoding operation types in the 32-bit word format:\n/// - MSB set to 1 for ABSORB operations\n/// - MSB set to 0 for SQUEEZE operations\n\n/// Flag for ABSORB operations (MSB = 1)\nglobal ABSORB_FLAG: u32 = 0x80000000;\n\n/// Flag for SQUEEZE operations (MSB = 0)\nglobal SQUEEZE_FLAG: u32 = 0x00000000;\n\n/// SAFE Sponge State (following spec 2.2)\n///\n/// The sponge state consists of the permutation state, tag, position counters,\n/// and IO pattern tracking as defined in the SAFE specification.\n///\n/// # Generic Parameters\n/// - `L`: The length of the IO pattern array\n///\n/// # Fields\n/// - `state`: Permutation state V in F^n (rate + capacity elements)\n/// - `tag`: Parameter tag T used for instance differentiation\n/// - `absorb_pos`: Current absorb position (<= n-c)\n/// - `squeeze_pos`: Current squeeze position (<= n-c)\n/// - `io_pattern`: Expected IO pattern for validation (encoded 32-bit words)\n/// - `io_count`: Current operation count for pattern tracking\npub struct SafeSponge<let L: u32> {\n    /// Permutation state V in F^n (rate + capacity elements).\n    state: [Field; STATE_SIZE],\n    /// Parameter tag T used for instance differentiation.\n    tag: Field,\n    /// Current absorb position (<= n-c).\n    absorb_pos: u32,\n    /// Current squeeze position (<= n-c).\n    squeeze_pos: u32,\n    /// Expected IO pattern for validation.\n    io_pattern: [u32; L],\n    /// Current operation count for pattern tracking (spec 2.4: io_count).\n    io_count: u32,\n}\n\nimpl<let L: u32> SafeSponge<L> {\n    /// Initializes a new SAFE sponge instance with the given IO pattern and domain separator (following spec 2.4).\n    ///\n    /// # Arguments\n    /// - `io_pattern`: Array of 32-bit encoded operations defining the expected sequence of ABSORB/SQUEEZE calls.\n    ///               Each word has MSB=1 for ABSORB operations, MSB=0 for SQUEEZE operations.\n    /// - `domain_separator`: 64-byte domain separator for cross-protocol security.\n    ///\n    /// # Returns\n    /// A new `SafeSponge` instance with initialized state\n    pub fn start(io_pattern: [u32; L], domain_separator: [u8; 64]) -> SafeSponge<L> {\n        // Compute tag from IO pattern and domain separator (spec 2.3).\n        let tag = compute_tag(io_pattern, domain_separator);\n\n        let mut state = [0; STATE_SIZE];\n        // Initialize capacity with tag (spec 2.4).\n        // Add T to the first 128 bits of the state.\n        state[0] = tag;\n\n        SafeSponge { state, tag, absorb_pos: 0, squeeze_pos: 0, io_pattern, io_count: 0 }\n    }\n\n    /// Absorbs field elements into the sponge state, interleaving permutation calls as needed (following spec 2.4).\n    ///\n    /// The number of elements to absorb is automatically validated against the IO pattern.\n    /// This method accepts variable-length arrays, making it natural to use without padding.\n    ///\n    /// # Arguments\n    /// - `input`: Array of field elements to absorb (variable length, must match IO pattern)\n    pub fn absorb(&mut self, input: Vec<Field>) {\n        let length = input.len() as u32;\n\n        // Validate against IO pattern.\n        assert(self.io_count < L);\n\n        // Parse expected operation from io_pattern (encoded word)\n        let expected_encoded_word = self.io_pattern[self.io_count];\n        let is_expected_absorb = (expected_encoded_word & ABSORB_FLAG) != 0;\n        let expected_length = expected_encoded_word & 0x7FFFFFFF;\n\n        // Validate operation type and length\n        assert(is_expected_absorb, \"Expected ABSORB operation\");\n        assert(expected_length == length, \"Length mismatch\");\n\n        // Process each element naturally (no unnecessary iterations).\n        for i in 0..length {\n            // If absorb_pos == (n-c) then permute and reset (spec 2.4).\n            if self.absorb_pos == RATE {\n                // n-c = RATE.\n                self.state = self.permute();\n                self.absorb_pos = 0;\n            }\n\n            // Add X[i] to state at absorb_pos (spec 2.4).\n            // Note: absorb_pos is the rate position, not capacity position.\n            self.state[self.absorb_pos + CAPACITY] =\n                self.state[self.absorb_pos + CAPACITY] + input.get(i);\n            self.absorb_pos += 1;\n        }\n\n        // Verify that the encoded word matches the expected pattern.\n        let encoded_word = ABSORB_FLAG | length;\n        assert(encoded_word == expected_encoded_word);\n\n        self.io_count += 1;\n\n        // Force permute at start of next SQUEEZE (spec 2.4).\n        self.squeeze_pos = RATE;\n    }\n\n    /// Extracts field elements from the sponge state, interleaving permutation calls as needed (following spec 2.4).\n    ///\n    /// The number of elements to squeeze is automatically determined from the IO pattern.\n    pub fn squeeze(&mut self) -> Vec<Field> {\n        // Validate against IO pattern.\n        assert(self.io_count < L);\n\n        // Parse expected operation from io_pattern (encoded word)\n        let expected_encoded_word = self.io_pattern[self.io_count];\n        let is_expected_squeeze = (expected_encoded_word & ABSORB_FLAG) == 0;\n        let length = expected_encoded_word & 0x7FFFFFFF;\n\n        // Validate operation type\n        assert(is_expected_squeeze, \"Expected SQUEEZE operation\");\n\n        let mut output = Vec::new();\n\n        // SQUEEZE implementation following spec 2.4.\n        // If length==0, loop won't execute (spec 2.4).\n        for _ in 0..length {\n            // If squeeze_pos==(n-c) then permute and reset (spec 2.4).\n            if self.squeeze_pos == RATE {\n                // n-c = RATE.\n                self.state = self.permute();\n                self.squeeze_pos = 0;\n                self.absorb_pos = 0;\n            }\n            // Set Y[i] to state element at squeeze_pos (spec 2.4).\n            output.push(self.state[self.squeeze_pos + CAPACITY]);\n            self.squeeze_pos += 1;\n        }\n\n        // Verify that the encoded word matches the expected pattern.\n        let encoded_word = SQUEEZE_FLAG | length;\n        assert(encoded_word == expected_encoded_word);\n\n        self.io_count += 1;\n        output\n    }\n\n    /// Finalizes the sponge instance, verifying that all expected operations have been performed and clearing the internal state for security (following spec 2.4).\n    ///\n    /// This function is used to ensure that the sponge instance has been used correctly and to prevent information leakage.\n    pub fn finish(&mut self) {\n        // Check that io_count equals the length of the IO pattern expected (spec 2.4).\n        assert(self.io_count == L, \"IO pattern not completed\");\n\n        // Erase the state and its variables (spec 2.4).\n        self.state = [0; STATE_SIZE];\n        self.absorb_pos = 0;\n        self.squeeze_pos = 0;\n        self.io_count = 0;\n    }\n\n    /// Permute the state using Poseidon2 (following spec 2.4).\n    ///\n    /// Applies the Poseidon2 permutation to the current state.\n    /// This is the core cryptographic primitive of the sponge construction.\n    ///\n    /// # Returns\n    /// New state after permutation\n    fn permute(self) -> [Field; STATE_SIZE] {\n        poseidon2_permutation(self.state, STATE_SIZE)\n    }\n}\n\n/// Computes a unique tag for a sponge instance based on its IO pattern and domain separator.\n/// The tag is used to ensure that distinct instances behave like distinct functions.\n///\n/// # Arguments\n/// - `io_pattern`: Array of 32-bit encoded operations defining the sponge's usage pattern.\n///               Each word has MSB=1 for ABSORB operations, MSB=0 for SQUEEZE operations.\n/// - `domain_separator`: 64-byte domain separator for cross-protocol security.\n///\n/// # Returns\n/// A field element representing the 128-bit tag.\npub fn compute_tag<let L: u32>(io_pattern: [u32; L], domain_separator: [u8; 64]) -> Field {\n    // Step 1: Parse and aggregate consecutive operations of the same type\n    let mut encoded_words = [0; L]; // Support up to L operations.\n    let mut word_count = 0;\n    let mut current_absorb_sum = 0;\n    let mut current_squeeze_sum = 0;\n    let mut last_was_absorb = false;\n\n    for i in 0..L {\n        if io_pattern[i] > 0 {\n            // Parse operation type from MSB and length from lower 31 bits\n            let is_absorb = (io_pattern[i] & ABSORB_FLAG) != 0;\n            let length = io_pattern[i] & 0x7FFFFFFF; // Clear MSB to get length\n\n            if is_absorb {\n                if last_was_absorb {\n                    // Aggregate consecutive ABSORB operations\n                    current_absorb_sum += length;\n                } else {\n                    // Start new ABSORB sequence\n                    if current_squeeze_sum > 0 {\n                        // Flush previous SQUEEZE sequence\n                        encoded_words[word_count] = SQUEEZE_FLAG | current_squeeze_sum;\n                        word_count += 1;\n                        current_squeeze_sum = 0;\n                    }\n                    current_absorb_sum = length;\n                }\n                last_was_absorb = true;\n            } else {\n                if !last_was_absorb {\n                    // Aggregate consecutive SQUEEZE operations\n                    current_squeeze_sum += length;\n                } else {\n                    // Start new SQUEEZE sequence\n                    if current_absorb_sum > 0 {\n                        // Flush previous ABSORB sequence\n                        encoded_words[word_count] = ABSORB_FLAG | current_absorb_sum;\n                        word_count += 1;\n                        current_absorb_sum = 0;\n                    }\n                    current_squeeze_sum = length;\n                }\n                last_was_absorb = false;\n            }\n        }\n    }\n\n    // Flush remaining operations\n    if current_absorb_sum > 0 {\n        encoded_words[word_count] = ABSORB_FLAG | current_absorb_sum;\n        word_count += 1;\n    }\n    if current_squeeze_sum > 0 {\n        encoded_words[word_count] = SQUEEZE_FLAG | current_squeeze_sum;\n        word_count += 1;\n    }\n\n    // Step 2: Serialize to byte string and append domain separator (following SAFE spec 2.3).\n    // Buffer is 256 bytes: max 192 bytes for IO pattern (48 words) + 64 bytes for domain separator.\n    // Note: We must use a fixed-size array because Noir's keccak256 requires [u8; N], not Vec<u8>.\n    let max_io_pattern_bytes: u32 = 192; // 256 - 64 (domain separator)\n    let io_pattern_bytes = word_count * 4;\n    assert(\n        io_pattern_bytes <= max_io_pattern_bytes,\n        \"IO pattern too large: max 48 aggregated words supported\",\n    );\n\n    let mut input_bytes = [0u8; 256];\n    let mut byte_count: u32 = 0;\n\n    // Serialize encoded words to bytes (big-endian as per SAFE spec).\n    // Note: Noir requires compile-time loop bounds, so we iterate over L (the array size)\n    // instead of word_count (runtime value). The condition `i < word_count` ensures we only\n    // process valid encoded words. This is safe because word_count <= L always holds\n    // (we can have at most L encoded words from L input operations).\n    for i in 0..L {\n        if i < word_count {\n            let word = encoded_words[i];\n            input_bytes[byte_count] = (word >> 24) as u8;\n            input_bytes[byte_count + 1] = (word >> 16) as u8;\n            input_bytes[byte_count + 2] = (word >> 8) as u8;\n            input_bytes[byte_count + 3] = word as u8;\n            byte_count += 4;\n        }\n    }\n\n    // Append full 64-byte domain separator.\n    for i in 0..64 {\n        input_bytes[byte_count] = domain_separator[i];\n        byte_count += 1;\n    }\n\n    // Step 3: Hash with Keccak-256 and truncate to 128 bits.\n    // Note: The SAFE spec uses SHA3-256, but we use Keccak-256 for Noir compatibility.\n    // Keccak-256 differs from SHA3-256 in padding, but both provide equivalent security.\n    let hash_bytes = keccak256(input_bytes, byte_count);\n\n    // Convert first 128 bits (16 bytes) to field element.\n    let mut tag_value: Field = 0;\n    for i in 0..16 {\n        tag_value = tag_value * 256 + (hash_bytes[i] as Field);\n    }\n\n    tag_value\n}\n\n#[test]\nfn test_safe_hashing() {\n    // Verifies basic hash functionality with a simple ABSORB(3) + SQUEEZE(1) pattern.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let elements = Vec::from_slice(&[1, 2, 3]);\n\n    // Pattern: ABSORB(3), SQUEEZE(1)\n    let io_pattern = [0x80000003, 0x00000001];\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(elements);\n    let output = sponge.squeeze();\n    sponge.finish();\n\n    assert(output.len() == 1);\n    assert(output.get(0) != 0);\n\n    // Test determinism\n    let mut sponge2 = SafeSponge::start(io_pattern, domain_separator);\n    sponge2.absorb(elements);\n    let output2 = sponge2.squeeze();\n    sponge2.finish();\n\n    assert(output2.len() == 1);\n    assert(output2.get(0) != 0);\n}\n\n#[test]\nfn test_merkle_node() {\n    // Verifies SAFE can be used for Merkle tree node hashing with pattern ABSORB(1) + ABSORB(1) + SQUEEZE(1).\n    // Tests the ability to absorb multiple inputs before squeezing output.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let left = Vec::from_slice([123]);\n    let right = Vec::from_slice([456]);\n\n    // Pattern: ABSORB(1), ABSORB(1), SQUEEZE(1)\n    let io_pattern = [0x80000001, 0x80000001, 0x00000001];\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(left);\n    sponge.absorb(right);\n    let output = sponge.squeeze();\n    sponge.finish();\n\n    assert(output.len() == 1);\n    assert(output.get(0) != 0);\n\n    // Test determinism\n    let mut sponge2 = SafeSponge::start(io_pattern, domain_separator);\n    sponge2.absorb(left);\n    sponge2.absorb(right);\n    let output2 = sponge2.squeeze();\n    sponge2.finish();\n\n    assert(output2.len() == 1);\n    assert(output2.get(0) != 0);\n}\n\n#[test]\nfn test_commitment_scheme() {\n    // Verifies SAFE can be used for commitment schemes with pattern ABSORB(3) + SQUEEZE(1).\n    // Tests the ability to create deterministic commitments from multiple field elements.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let values = Vec::from_slice([10, 20, 30]);\n\n    // Pattern: ABSORB(3), SQUEEZE(1)\n    let io_pattern = [0x80000003, 0x00000001];\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(values);\n    let output = sponge.squeeze();\n    sponge.finish();\n\n    assert(output.len() == 1);\n    assert(output.get(0) != 0);\n\n    // Test determinism\n    let mut sponge2 = SafeSponge::start(io_pattern, domain_separator);\n    sponge2.absorb(values);\n    let output2 = sponge2.squeeze();\n    sponge2.finish();\n\n    assert(output2.len() == 1);\n    assert(output2.get(0) != 0);\n}\n\n#[test]\nfn test_domain_separation() {\n    // Verifies that different domain separators produce different outputs for the same input.\n    // This is crucial for cross-protocol security and preventing collisions between different applications.\n    let elements = Vec::from_slice([1, 2, 3]);\n    let domain1 = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let domain2 = [\n        0x41, 0x42, 0x43, 0x45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Pattern: ABSORB(3), SQUEEZE(1)\n    let io_pattern = [0x80000003, 0x00000001];\n\n    let mut sponge1 = SafeSponge::start(io_pattern, domain1);\n    sponge1.absorb(elements);\n    let output1 = sponge1.squeeze();\n    sponge1.finish();\n\n    let mut sponge2 = SafeSponge::start(io_pattern, domain2);\n    sponge2.absorb(elements);\n    let output2 = sponge2.squeeze();\n    sponge2.finish();\n\n    assert(output1.len() == 1);\n    assert(output2.len() == 1);\n    assert(output1.get(0) != output2.get(0)); // Different domain separators should produce different outputs\n}\n\n#[test]\nfn test_multiple_squeeze() {\n    // Verifies that multiple field elements can be squeezed in a single operation.\n    // Tests pattern ABSORB(3) + SQUEEZE(2) to ensure proper state management.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n    let elements = Vec::from_slice([1, 2, 3]);\n\n    // Pattern: ABSORB(3), SQUEEZE(2)\n    let io_pattern = [0x80000003, 0x00000002];\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(elements);\n    let output = sponge.squeeze();\n    sponge.finish();\n\n    assert(output.len() == 2);\n    assert(output.get(0) != 0);\n    assert(output.get(1) != 0);\n    assert(output.get(0) != output.get(1)); // Different squeeze outputs should be different\n}\n\n#[test]\nfn test_zero_length_operations() {\n    // Verifies that zero-length ABSORB and SQUEEZE operations are handled correctly.\n    // Tests pattern ABSORB(0) + SQUEEZE(1) to ensure proper state transitions.\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Pattern: ABSORB(0), SQUEEZE(1)\n    let io_pattern = [0x80000000, 0x00000001];\n    let mut sponge = SafeSponge::start(io_pattern, domain_separator);\n    sponge.absorb(Vec::new());\n    let output = sponge.squeeze();\n    sponge.finish();\n\n    assert(output.len() == 1);\n    assert(output.get(0) != 0);\n}\n\n#[test]\nfn test_tag_computation() {\n    // Verifies the tag computation algorithm using the example from the SAFE specification.\n    // Pattern: ABSORB(3), ABSORB(3), SQUEEZE(3)\n    // Should aggregate to: ABSORB(6), SQUEEZE(3)\n    // Encoded as: [0x80000006, 0x00000003]\n    // Tests determinism and pattern differentiation.\n\n    let io_pattern = [0x80000003, 0x80000003, 0x00000003];\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    let tag = compute_tag(io_pattern, domain_separator);\n\n    // Test determinism\n    let tag2 = compute_tag(io_pattern, domain_separator);\n    assert(tag == tag2);\n\n    // Test that different patterns produce different tags\n    let io_pattern2 = [0x80000003, 0x00000003]; // ABSORB(3), SQUEEZE(3) - different pattern\n    let tag3 = compute_tag(io_pattern2, domain_separator);\n    assert(tag != tag3);\n}\n\n#[test]\nfn test_tag_computation_debug() {\n    println(\"=== SAFE Tag Computation Debug Test ===\");\n\n    // Test your specific pattern [2, 2, 2] (ABSORB(2), SQUEEZE(2), ABSORB(2))\n    let io_pattern = [0x80000002, 0x00000002, 0x80000002];\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    println(f\"Testing pattern: {io_pattern}\");\n    println(\n        f\"Expected to aggregate to: ABSORB(2), SQUEEZE(2), ABSORB(2)\",\n    );\n    println(\n        f\"Expected encoded words: [0x80000002, 0x00000002, 0x80000002]\",\n    );\n    println(\"\");\n\n    let tag = compute_tag(io_pattern, domain_separator);\n\n    println(f\"=== Expected Rust Output ===\");\n    println(\"Pattern [2, 2, 2] (ABSORB(2), SQUEEZE(2), ABSORB(2))\");\n    println(\"Domain separator: 0x41424344...\");\n    println(\"Tag: 0xce3bb9ee4b2d41c42e9cdda38afe8b6a\");\n    println(\"\");\n\n    println(f\"=== Noir Output ===\");\n    println(f\"Tag: {tag}\");\n    println(\"\");\n\n    println(\"Compare the tag values above with Rust script!\");\n}\n\n#[test]\nfn test_consecutive_absorb_aggregation() {\n    // Test that consecutive ABSORB operations are properly aggregated\n    // Pattern: ABSORB(1), ABSORB(1), SQUEEZE(1) should aggregate to ABSORB(2), SQUEEZE(1)\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Test pattern: ABSORB(1), ABSORB(1), SQUEEZE(1)\n    let io_pattern = [0x80000001, 0x80000001, 0x00000001];\n\n    // This should aggregate to: ABSORB(2), SQUEEZE(1) = [0x80000002, 0x00000001]\n    let tag = compute_tag(io_pattern, domain_separator);\n\n    // Test that the aggregated pattern produces the same tag ABSORB(2), SQUEEZE(1)\n    let aggregated_pattern = [0x80000002, 0x00000001];\n    let aggregated_tag = compute_tag(aggregated_pattern, domain_separator);\n\n    // The tags should be identical because the patterns are equivalent after aggregation\n    assert(tag == aggregated_tag, \"Consecutive ABSORB operations should aggregate to the same tag\");\n\n    // Test that a different pattern produces a different tag\n    let different_pattern = [0x80000001, 0x00000001, 0x80000001]; // ABSORB(1), SQUEEZE(1), ABSORB(1)\n    let different_tag = compute_tag(different_pattern, domain_separator);\n\n    // This should be different because it doesn't have consecutive ABSORB operations\n    assert(tag != different_tag, \"Different patterns should produce different tags\");\n\n    println(\"=== Consecutive ABSORB Aggregation Test ===\");\n    println(\n        f\"Original pattern: [0x80000001, 0x80000001, 0x00000001] (ABSORB(1), ABSORB(1), SQUEEZE(1))\",\n    );\n    println(\n        f\"Aggregated pattern: [0x80000002, 0x00000001] (ABSORB(2), SQUEEZE(1))\",\n    );\n    println(f\"Original tag: {tag}\");\n    println(f\"Aggregated tag: {aggregated_tag}\");\n    println(f\"Original tag: {tag}\");\n    println(f\"Aggregated tag: {aggregated_tag}\");\n    println(f\"Different pattern tag: {different_tag}\");\n}\n\n#[test]\nfn test_consecutive_squeeze_aggregation() {\n    // Test that consecutive SQUEEZE operations are properly aggregated\n    // Pattern: ABSORB(1), SQUEEZE(1), SQUEEZE(1) should aggregate to ABSORB(1), SQUEEZE(2)\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Test pattern: ABSORB(1), SQUEEZE(1), SQUEEZE(1)\n    let io_pattern = [0x80000001, 0x00000001, 0x00000001];\n\n    // This should aggregate to: ABSORB(1), SQUEEZE(2) = [0x80000001, 0x00000002]\n    let tag = compute_tag(io_pattern, domain_separator);\n\n    // Test that the aggregated pattern produces the same tag ABSORB(1), SQUEEZE(2)\n    let aggregated_pattern = [0x80000001, 0x00000002];\n    let aggregated_tag = compute_tag(aggregated_pattern, domain_separator);\n\n    // The tags should be identical because the patterns are equivalent after aggregation\n    assert(\n        tag == aggregated_tag,\n        \"Consecutive SQUEEZE operations should aggregate to the same tag\",\n    );\n\n    // Test that a different pattern produces a different tag\n    let different_pattern = [0x80000001, 0x00000001, 0x80000001]; // ABSORB(1), SQUEEZE(1), ABSORB(1)\n    let different_tag = compute_tag(different_pattern, domain_separator);\n\n    // This should be different because it doesn't have consecutive SQUEEZE operations\n    assert(tag != different_tag, \"Different patterns should produce different tags\");\n\n    println(\"=== Consecutive SQUEEZE Aggregation Test ===\");\n    println(\n        f\"Original pattern: [0x80000001, 0x00000001, 0x00000001] (ABSORB(1), SQUEEZE(1), SQUEEZE(1))\",\n    );\n    println(\n        f\"Aggregated pattern: [0x80000001, 0x00000002] (ABSORB(1), SQUEEZE(2))\",\n    );\n    println(f\"Original tag: {tag}\");\n    println(f\"Aggregated tag: {aggregated_tag}\");\n    println(f\"Different pattern tag: {different_tag}\");\n}\n\n#[test]\nfn test_mixed_consecutive_aggregation() {\n    // Test that both consecutive ABSORB and SQUEEZE operations are properly aggregated\n    // Pattern: ABSORB(1), ABSORB(1), SQUEEZE(1), SQUEEZE(1), ABSORB(1)\n    // Should aggregate to: ABSORB(2), SQUEEZE(2), ABSORB(1)\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Test pattern: ABSORB(1), ABSORB(1), SQUEEZE(1), SQUEEZE(1), ABSORB(1)\n    let io_pattern = [0x80000001, 0x80000001, 0x00000001, 0x00000001, 0x80000001];\n\n    // This should aggregate to: ABSORB(2), SQUEEZE(2), ABSORB(1) = [0x80000002, 0x00000002, 0x80000001]\n    let tag = compute_tag(io_pattern, domain_separator);\n\n    // Test that the aggregated pattern produces the same tag\n    let aggregated_pattern = [0x80000002, 0x00000002, 0x80000001]; // ABSORB(2), SQUEEZE(2), ABSORB(1)\n    let aggregated_tag = compute_tag(aggregated_pattern, domain_separator);\n\n    // The tags should be identical because the patterns are equivalent after aggregation\n    assert(tag == aggregated_tag, \"Mixed consecutive operations should aggregate to the same tag\");\n\n    println(\"=== Mixed Consecutive Aggregation Test ===\");\n    println(\n        f\"Original pattern: [0x80000001, 0x80000001, 0x00000001, 0x00000001, 0x80000001]\",\n    );\n    println(\n        f\"  (ABSORB(1), ABSORB(1), SQUEEZE(1), SQUEEZE(1), ABSORB(1))\",\n    );\n    println(f\"Aggregated pattern: [0x80000002, 0x00000002, 0x80000001]\");\n    println(f\"  (ABSORB(2), SQUEEZE(2), ABSORB(1))\");\n    println(f\"Original tag: {tag}\");\n    println(f\"Aggregated tag: {aggregated_tag}\");\n}\n\n#[test]\nfn test_large_io_pattern() {\n    let domain_separator = [\n        0x41, 0x42, 0x43, 0x44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0,\n    ];\n\n    // Create pattern with 48 alternating ABSORB(1) and SQUEEZE(1) operations\n    // This is the maximum supported (48 words * 4 bytes = 192 bytes, leaving 64 for domain separator)\n    let mut io_pattern = [0u32; 48];\n    for i in 0..48 {\n        if i % 2 == 0 {\n            io_pattern[i] = ABSORB_FLAG | 1; // ABSORB(1)\n        } else {\n            io_pattern[i] = SQUEEZE_FLAG | 1; // SQUEEZE(1)\n        }\n    }\n\n    let tag = compute_tag(io_pattern, domain_separator);\n    assert(tag != 0);\n}\n\n#[test]\nfn test_domain_separator_not_truncated() {\n    // This test verifies that the domain separator is always included in the tag computation,\n    // even for large IO patterns. If the domain separator were truncated, different domain\n    // separators would produce the same tag for large patterns.\n\n    let domain_separator_a = [\n        0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41,\n        0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41,\n        0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41,\n        0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41, 0x41,\n        0x41, 0x41, 0x41, 0x41,\n    ]; // All 'A's\n\n    let domain_separator_b = [\n        0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42,\n        0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42,\n        0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42,\n        0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42, 0x42,\n        0x42, 0x42, 0x42, 0x42,\n    ]; // All 'B's\n\n    // Create pattern with 48 alternating operations (max supported: 192 bytes of IO pattern)\n    let mut io_pattern = [0u32; 48];\n    for i in 0..48 {\n        if i % 2 == 0 {\n            io_pattern[i] = ABSORB_FLAG | 1;\n        } else {\n            io_pattern[i] = SQUEEZE_FLAG | 1;\n        }\n    }\n\n    let tag_a = compute_tag(io_pattern, domain_separator_a);\n    let tag_b = compute_tag(io_pattern, domain_separator_b);\n\n    // Tags MUST be different because domain separators are different.\n    // If they were the same, it would mean the domain separator was truncated/ignored.\n    assert(tag_a != tag_b, \"Domain separator must affect tag even for large IO patterns\");\n}\n","path":"enclave/circuits/lib/src/math/safe.nr"}},"expression_width":{"Bounded":{"width":4}}}